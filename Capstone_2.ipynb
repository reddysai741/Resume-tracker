{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAoCiZ6zxupN",
        "outputId": "98477723-8fa2-4b04-af53-7418708e1109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.24.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n",
            "Downloading groq-0.24.0-py3-none-any.whl (127 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/127.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m122.9/127.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.5/127.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.24.0\n"
          ]
        }
      ],
      "source": [
        "!pip install groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lOG1AyOx5t-",
        "outputId": "801c43b3-1d3b-45f6-c8c1-d998fcbf5c46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.3.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain-groq) (0.3.59)\n",
            "Requirement already satisfied: groq<1,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from langchain-groq) (0.24.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.13.2)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (0.3.42)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-groq) (24.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (3.10.18)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-groq) (2.4.0)\n",
            "Downloading langchain_groq-0.3.2-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: langchain-groq\n",
            "Successfully installed langchain-groq-0.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(\n",
        "    temperature=0,\n",
        "    groq_api_key='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',\n",
        "    model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\"\n",
        ")\n",
        "\n",
        "response = llm.invoke(\"The first person to land on moon was ...\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In2-5OiDx_2T",
        "outputId": "45ff89d0-b83a-4407-cd0f-b992e26aa531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best answer is  Neil Armstrong\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvxM70LxyD3A",
        "outputId": "0672f72b-d3af-497b-fef0-af74b3c037a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/232.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import re\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "ItBF07BuyIZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  pdfFileObj=open(\"/content/Sainath Reddy.pdf\",'rb')\n",
        "  pdfReader=PyPDF2.PdfReader(pdfFileObj)\n",
        "  for page in pdfReader.pages:\n",
        "    pagecontent=page.extract_text()\n",
        "    print(pagecontent)\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLgZ7pK3yLLa",
        "outputId": "fcec59c6-370a-44a0-fbac-0e56f255f0c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SAINATH REDDY\n",
            "/githubgithub.com/Sainath reddy /linkedinSainath reddy /envel⌢pereddysainath741@gmail.com ♂phone9606155390\n",
            "Experience\n",
            "Imarticus Learning |Data Science Intern Jan 2025 - April 2025\n",
            "Enhancing Placement Analytics and Alumni Network Optimization\n",
            "•Designed and implemented a system to track students’ placement details using web scraping, Selenium, and\n",
            "LinkedIn API, with a user-friendly frontend interface to fetch and manage details like company name, job\n",
            "title, experience, and joining information (internship or full-time).\n",
            "•Developed a comprehensive job tracking module to capture key attributes such as field, experience, location,\n",
            "job description, and required skills for diverse opportunities\n",
            "•Automated information extraction from job portals to streamline data collection and analysis.\n",
            "•Built an AI-driven model to align training and skill development with job descriptions for personalized\n",
            "recommendations.\n",
            "•Created a process for generating customized email communications to HR for enhanced outreach and\n",
            "placement support\n",
            "Education\n",
            "Presidency University, Bangalore, Karnataka June 2024\n",
            "B.Tech Computer Science and Engineering (Data Science)\n",
            "Vision PU College,Bangalore, Karnataka June 2019\n",
            "Science (PCMB)\n",
            "Sri SiddhiVinayaka Residential School,Udupi, Karnataka May 2017\n",
            "Class 10\n",
            "Coursework\n",
            "Data Science, Statistics, Machine Learning Algorithms, Deep Learning, NLP, Data Analysis, Data Mining,OS\n",
            "Data Visualization, Predictive Analytics & Modeling, MySQL,AWS(SageMaker,EC2)\n",
            "Skills\n",
            "Programming Languages : Python, HTML/CSS,Java\n",
            "Tools : Git/GitHub, VS Code, IntelliJ IDEA, Eclipse, PyCharm, Jupyter Notebook, Tableau,PowerBI\n",
            "Interpersonal Skills : Communication (Written & Verbal), Active Listening, Teamwork, Leadership,\n",
            "Responsibility\n",
            "Projects\n",
            "Synchronizing supply chains with data intelligence. |PowerBI,MySQL,ETL,Excel,ML models April 2025\n",
            "•Developed a predictive analytics solution leveraging machine learning models (Random Forest, Linear SVC,\n",
            "Gradient Descent), SQL, exploratory data analysis (EDA), and Power BI to assess and minimize late delivery\n",
            "risks in a global supply chain network.\n",
            "Text to SQL LLM-Application |Python,MySQL Server,LLM-Model,Streamlit,API March. 2024\n",
            "•This application leverages Google’s Gemini Pro to convert natural language queries into SQL statements,\n",
            "employs Streamlit for an intuitive user interface, and interacts with an SQLite database to retrieve relevant\n",
            "information.\n",
            "Satellite Image Classification using ML Algorithms |Python, Machine Learning, Image Processing May 2023\n",
            "•Employed image processing techniques and machine learning algorithms to classify satellite images, extracting\n",
            "relevant features as inputs to the models.\n",
            "Certificates\n",
            "Masters Program in Data Science |NASSCOM April 2025\n",
            "Data Science Certification |Imarticus Learning Oct 2024\n",
            "Privacy and Security in Online Social Media |NPTEL - IIIT Hyderabad Nov. 2023\n",
            "Data Analysis with Pandas, NumPy and PostgreSQL |Skill Vertex July 2023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "prompt_extract = PromptTemplate.from_template(\n",
        "        \"\"\"\n",
        "        ### SCRAPED TEXT FROM WEBSITE:\n",
        "        {pagecontent}\n",
        "        ### INSTRUCTION:\n",
        "        i will give my resume from that extract the name,experience but in this dont take description ,techincal skills in jason format , No preamble\n",
        "        Only return the valid JSON.\n",
        "        ### VALID JSON (NO PREAMBLE):\n",
        "        \"\"\"\n",
        ")\n",
        "\n",
        "chain_extract = prompt_extract | llm\n",
        "resume = chain_extract.invoke(input={'pagecontent':pagecontent})\n",
        "print(resume.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vWV-p-8yW_H",
        "outputId": "51e9aa0b-e10b-4588-b79e-fbae634cff3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"name\": \"SAINATH REDDY\",\n",
            "  \"experience\": [\n",
            "    {\n",
            "      \"company\": \"Imarticus Learning\",\n",
            "      \"position\": \"Data Science Intern\",\n",
            "      \"duration\": \"Jan2025 - April2025\"\n",
            "    }\n",
            "  ],\n",
            "  \"education\": [\n",
            "    {\n",
            "      \"institution\": \"Presidency University, Bangalore, Karnataka\",\n",
            "      \"degree\": \"B.Tech Computer Science and Engineering (Data Science)\",\n",
            "      \"duration\": \"June2024\"\n",
            "    },\n",
            "    {\n",
            "      \"institution\": \"Vision PU College,Bangalore, Karnataka\",\n",
            "      \"degree\": \"Science (PCMB)\",\n",
            "      \"duration\": \"June2019\"\n",
            "    },\n",
            "    {\n",
            "      \"institution\": \"Sri SiddhiVinayaka Residential School,Udupi, Karnataka\",\n",
            "      \"degree\": \"Class10\",\n",
            "      \"duration\": \"May2017\"\n",
            "    }\n",
            "  ],\n",
            "  \"technical_skills\": {\n",
            "    \"programming_languages\": [\"Python\", \"HTML/CSS\", \"Java\"],\n",
            "    \"tools\": [\"Git/GitHub\", \"VS Code\", \"IntelliJ IDEA\", \"Eclipse\", \"PyCharm\", \"Jupyter Notebook\", \"Tableau\", \"PowerBI\"],\n",
            "    \"interpersonal_skills\": [\"Communication (Written & Verbal)\", \"Active Listening\", \"Teamwork\", \"Leadership\", \"Responsibility\"],\n",
            "    \"coursework\": [\"Data Science\", \"Statistics\", \"Machine Learning Algorithms\", \"Deep Learning\", \"NLP\", \"Data Analysis\", \"Data Mining\", \"OS\", \"Data Visualization\", \"Predictive Analytics & Modeling\", \"MySQL\", \"AWS(SageMaker,EC2)\"]\n",
            "  },\n",
            "  \"projects\": [\n",
            "    {\n",
            "      \"title\": \"Synchronizing supply chains with data intelligence.\",\n",
            "      \"tech_used\": [\"PowerBI\", \"MySQL\", \"ETL\", \"Excel\", \"ML models\"],\n",
            "      \"duration\": \"April2025\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Text to SQL LLM-Application\",\n",
            "      \"tech_used\": [\"Python\", \"MySQL Server\", \"LLM-Model\", \"Streamlit\", \"API\"],\n",
            "      \"duration\": \"March.2024\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Satellite Image Classification using ML Algorithms\",\n",
            "      \"tech_used\": [\"Python\", \"Machine Learning\", \"Image Processing\"],\n",
            "      \"duration\": \"May2023\"\n",
            "    }\n",
            "  ],\n",
            "  \"certificates\": [\n",
            "    {\n",
            "      \"title\": \"Masters Program in Data Science\",\n",
            "      \"issuer\": \"NASSCOM\",\n",
            "      \"duration\": \"April2025\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Data Science Certification\",\n",
            "      \"issuer\": \"Imarticus Learning\",\n",
            "      \"duration\": \"Oct2024\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Privacy and Security in Online Social Media\",\n",
            "      \"issuer\": \"NPTEL - IIIT Hyderabad\",\n",
            "      \"duration\": \"Nov.2023\"\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Data Analysis with Pandas, NumPy and PostgreSQL\",\n",
            "      \"issuer\": \"Skill Vertex\",\n",
            "      \"duration\": \"July2023\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "json_parser = JsonOutputParser()\n",
        "json_resume = json_parser.parse(resume.content)\n",
        "json_resume"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxiBQWLyycat",
        "outputId": "34a6a9fc-e240-497d-eb72-edbed8a4ead8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'SAINATH REDDY',\n",
              " 'experience': [{'company': 'Imarticus Learning',\n",
              "   'position': 'Data Science Intern',\n",
              "   'duration': 'Jan2025 - April2025'}],\n",
              " 'education': [{'institution': 'Presidency University, Bangalore, Karnataka',\n",
              "   'degree': 'B.Tech Computer Science and Engineering (Data Science)',\n",
              "   'duration': 'June2024'},\n",
              "  {'institution': 'Vision PU College,Bangalore, Karnataka',\n",
              "   'degree': 'Science (PCMB)',\n",
              "   'duration': 'June2019'},\n",
              "  {'institution': 'Sri SiddhiVinayaka Residential School,Udupi, Karnataka',\n",
              "   'degree': 'Class10',\n",
              "   'duration': 'May2017'}],\n",
              " 'technical_skills': {'programming_languages': ['Python', 'HTML/CSS', 'Java'],\n",
              "  'tools': ['Git/GitHub',\n",
              "   'VS Code',\n",
              "   'IntelliJ IDEA',\n",
              "   'Eclipse',\n",
              "   'PyCharm',\n",
              "   'Jupyter Notebook',\n",
              "   'Tableau',\n",
              "   'PowerBI'],\n",
              "  'interpersonal_skills': ['Communication (Written & Verbal)',\n",
              "   'Active Listening',\n",
              "   'Teamwork',\n",
              "   'Leadership',\n",
              "   'Responsibility'],\n",
              "  'coursework': ['Data Science',\n",
              "   'Statistics',\n",
              "   'Machine Learning Algorithms',\n",
              "   'Deep Learning',\n",
              "   'NLP',\n",
              "   'Data Analysis',\n",
              "   'Data Mining',\n",
              "   'OS',\n",
              "   'Data Visualization',\n",
              "   'Predictive Analytics & Modeling',\n",
              "   'MySQL',\n",
              "   'AWS(SageMaker,EC2)']},\n",
              " 'projects': [{'title': 'Synchronizing supply chains with data intelligence.',\n",
              "   'tech_used': ['PowerBI', 'MySQL', 'ETL', 'Excel', 'ML models'],\n",
              "   'duration': 'April2025'},\n",
              "  {'title': 'Text to SQL LLM-Application',\n",
              "   'tech_used': ['Python', 'MySQL Server', 'LLM-Model', 'Streamlit', 'API'],\n",
              "   'duration': 'March.2024'},\n",
              "  {'title': 'Satellite Image Classification using ML Algorithms',\n",
              "   'tech_used': ['Python', 'Machine Learning', 'Image Processing'],\n",
              "   'duration': 'May2023'}],\n",
              " 'certificates': [{'title': 'Masters Program in Data Science',\n",
              "   'issuer': 'NASSCOM',\n",
              "   'duration': 'April2025'},\n",
              "  {'title': 'Data Science Certification',\n",
              "   'issuer': 'Imarticus Learning',\n",
              "   'duration': 'Oct2024'},\n",
              "  {'title': 'Privacy and Security in Online Social Media',\n",
              "   'issuer': 'NPTEL - IIIT Hyderabad',\n",
              "   'duration': 'Nov.2023'},\n",
              "  {'title': 'Data Analysis with Pandas, NumPy and PostgreSQL',\n",
              "   'issuer': 'Skill Vertex',\n",
              "   'duration': 'July2023'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex8kKc5kyjGe",
        "outputId": "5c1b5777-a16a-41de-d9f9-3bbf92f3a6bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXDTGP6hymbU",
        "outputId": "aaea0ab1-9ea6-47ff-db2c-fcad3029d72f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdKqzh_OyrBC",
        "outputId": "95c675c3-9e6f-4dcd-80e9-057e8a5ec36f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = WebBaseLoader(\"https://amazon.jobs/en/jobs/2979106/applied-scientist-i-amazon-payments-team\")\n",
        "page_data = loader.load().pop().page_content\n",
        "print(page_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cxhIxdeyvWi",
        "outputId": "3aa838d9-1347-451a-d8c5-3f88b56e88a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied Scientist I, Amazon Payments Team - Job ID: 2979106 | Amazon.jobs\n",
            "Skip to main contentHomeTeamsLocationsJob categoriesMy careerMy applicationsMy profileAccount securitySettingsSign outResourcesDisability accommodationsBenefitsInclusive experiencesInterview tipsLeadership principlesWorking at AmazonFAQ×Applied Scientist I, Amazon Payments TeamJob ID: 2979106 | Amazon.com Services LLCApply nowDESCRIPTIONAmazon Payments build systems that process payments at an unprecedented scale, with accuracy, speed, and mission-critical availability. We process millions of transactions every day worldwide across various payment methods. Over 100 million customers and merchants send hundreds of billions of dollars moving at light-speed through our systems annually. We are looking for a highly skilled, experienced, and motivated Applied Scientist to innovate and solve complex scientific optimization challenges at a massive scale.This Applied Scientist role will design and implement state-of-the-art AI optimization models that generate multi-billion dollar predictions of the highest level of visibility and importance for Amazon's Payments and Customer Experience. The scientist will work on implementing Agentic workflows within production systems that will tremendously improve developer productivity at scale. A successful candidate will be a problem solver who enjoys diving into data, is excited by difficult modeling challenges, and possesses strong communication skills to effectively interface between technical and business teams. You will contribute to the research community by working with other scientists across Amazon and our Payments Engineering as well as by collaborating with academic researchers and publishing papers. You will work closely with Software Development Engineers to invent and construct models on data at massive scale and it is likely that your work will end up in an Amazon product. Finally, you will also have exposure to senior leadership as we communicate results and provide scientific guidance to the business.Key job responsibilitiesAs a Applied Scientist you will: - Drive collaborative research and creative problem solving.- Write production-grade quality code for deploying machine learning and deep learning modes.  - Constructively critique peer research and mentor junior scientists and engineers. - Create experiments and prototype implementations of state-of-the-art AI techniques.- Create agentic workflows using GenAI state-of-the-art techniques. - Collaborate with engineering teams to design and implement software solutions for science problems. - Contribute to progress of the Amazon and broader research communities by producing publication - Effectively collaborate in a fast paced environment with multiple teams in large organization (software development, Project Management, Build and Release, etc).About the teamAmazon Payments Machine Learning team  leverages data across the payment lifecycle to build ML capabilities to improve payment-operations' success rates for verification, authentication, authorization, settlement, refund, disbursement etc. The team also leads impactful generative AI initiatives, driving reductions in operations, an improved developer experience, and friction-less client interactions.BASIC QUALIFICATIONS- Master's degree in computer science, mathematics, statistics, machine learning or equivalent quantitative field- Experience programming in Java, C++, Python or related language- Experience with SQL and an RDBMS (e.g., Oracle) or Data Warehouse- Experience building machine learning models or developing algorithms for business applicationPREFERRED QUALIFICATIONS- Experience implementing algorithms using both toolkits and self-developed code- Have publications at top-tier peer-reviewed conferences or journals- PhD in computer science, mathematics, statistics, machine learning or equivalent quantitative field- Experience in solving business problems through machine learning, data mining and statistical algorithmsAmazon is an equal opportunity employer and does not discriminate on the basis of protected veteran status, disability, or other legally protected status.Our inclusive culture empowers Amazonians to deliver the best results for our customers. If you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. If the country/region you’re applying in isn’t listed, please contact your Recruiting Partner.Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $129,400/year in our lowest geographic market up to $212,800/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information,  please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.Job detailsUSA, WA, SeattlePayments EngineeringData ScienceShare this jobJOIN US ONFind CareersJob CategoriesTeamsLocationsUS and EU Military recruitingWarehouse and Hourly JobsWorking At AmazonCultureBenefitsAmazon NewsletterDiversity at AmazonOur leadership principlesHelpFAQInterview tipsReview application statusDisability accommodationsLegal disclosures and noticesAmazon is an equal opportunity employer and does not discriminate on the basis of protected veteran status, disability, or other legally protected status.Privacy and DataImpressum© 1996-2025, Amazon.com, Inc. or its affiliates\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "prompt_extract = PromptTemplate.from_template(\n",
        "        \"\"\"\n",
        "        ### SCRAPED TEXT FROM WEBSITE:\n",
        "        {page_data}\n",
        "        ### INSTRUCTION:\n",
        "        The scraped text is from the career's page of a website.\n",
        "        Your job is to extract the job postings and return them in JSON format containing the\n",
        "        following keys: `role`, `experience`, `skills` and `description`.\n",
        "        Only return the valid JSON.\n",
        "        ### VALID JSON (NO PREAMBLE):\n",
        "        \"\"\"\n",
        ")\n",
        "\n",
        "chain_extract = prompt_extract | llm\n",
        "job = chain_extract.invoke(input={'page_data':page_data})\n",
        "print(job.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIto_pJJyykx",
        "outputId": "add15ba2-f8e3-4fbd-c561-b0b8e5765672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"role\": \"Applied Scientist I, Amazon Payments Team\",\n",
            "  \"experience\": {\n",
            "    \"basic\": [\n",
            "      \"Master's degree in computer science, mathematics, statistics, machine learning or equivalent quantitative field\",\n",
            "      \"Experience programming in Java, C++, Python or related language\",\n",
            "      \"Experience with SQL and an RDBMS (e.g., Oracle) or Data Warehouse\",\n",
            "      \"Experience building machine learning models or developing algorithms for business application\"\n",
            "    ],\n",
            "    \"preferred\": [\n",
            "      \"Experience implementing algorithms using both toolkits and self-developed code\",\n",
            "      \"Have publications at top-tier peer-reviewed conferences or journals\",\n",
            "      \"PhD in computer science, mathematics, statistics, machine learning or equivalent quantitative field\",\n",
            "      \"Experience in solving business problems through machine learning, data mining and statistical algorithms\"\n",
            "    ]\n",
            "  },\n",
            "  \"skills\": [\n",
            "    \"Machine learning\",\n",
            "    \"Deep learning\",\n",
            "    \"Python\",\n",
            "    \"Java\",\n",
            "    \"C++\",\n",
            "    \"SQL\",\n",
            "    \"Data Warehouse\",\n",
            "    \"RDBMS\",\n",
            "    \"GenAI\",\n",
            "    \"Agentic workflows\"\n",
            "  ],\n",
            "  \"description\": \"Amazon Payments build systems that process payments at an unprecedented scale, with accuracy, speed, and mission-critical availability. We process millions of transactions every day worldwide across various payment methods. Over 100 million customers and merchants send hundreds of billions of dollars moving at light-speed through our systems annually. We are looking for a highly skilled, experienced, and motivated Applied Scientist to innovate and solve complex scientific optimization challenges at a massive scale. This Applied Scientist role will design and implement state-of-the-art AI optimization models that generate multi-billion dollar predictions of the highest level of visibility and importance for Amazon's Payments and Customer Experience. The scientist will work on implementing Agentic workflows within production systems that will tremendously improve developer productivity at scale. A successful candidate will be a problem solver who enjoys diving into data, is excited by difficult modeling challenges, and possesses strong communication skills to effectively interface between technical and business teams. You will contribute to the research community by working with other scientists across Amazon and our Payments Engineering as well as by collaborating with academic researchers and publishing papers. You will work closely with Software Development Engineers to invent and construct models on data at massive scale and it is likely that your work will end up in an Amazon product. Finally, you will also have exposure to senior leadership as we communicate results and provide scientific guidance to the business.\"\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "json_parser = JsonOutputParser()\n",
        "json_job = json_parser.parse(job.content)\n",
        "json_job"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrMlvRT9y9BN",
        "outputId": "ee7525ff-3b2d-4c78-d3d8-bd7a0255654c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'role': 'Applied Scientist I, Amazon Payments Team',\n",
              " 'experience': {'basic': [\"Master's degree in computer science, mathematics, statistics, machine learning or equivalent quantitative field\",\n",
              "   'Experience programming in Java, C++, Python or related language',\n",
              "   'Experience with SQL and an RDBMS (e.g., Oracle) or Data Warehouse',\n",
              "   'Experience building machine learning models or developing algorithms for business application'],\n",
              "  'preferred': ['Experience implementing algorithms using both toolkits and self-developed code',\n",
              "   'Have publications at top-tier peer-reviewed conferences or journals',\n",
              "   'PhD in computer science, mathematics, statistics, machine learning or equivalent quantitative field',\n",
              "   'Experience in solving business problems through machine learning, data mining and statistical algorithms']},\n",
              " 'skills': ['Machine learning',\n",
              "  'Deep learning',\n",
              "  'Python',\n",
              "  'Java',\n",
              "  'C++',\n",
              "  'SQL',\n",
              "  'Data Warehouse',\n",
              "  'RDBMS',\n",
              "  'GenAI',\n",
              "  'Agentic workflows'],\n",
              " 'description': \"Amazon Payments build systems that process payments at an unprecedented scale, with accuracy, speed, and mission-critical availability. We process millions of transactions every day worldwide across various payment methods. Over 100 million customers and merchants send hundreds of billions of dollars moving at light-speed through our systems annually. We are looking for a highly skilled, experienced, and motivated Applied Scientist to innovate and solve complex scientific optimization challenges at a massive scale. This Applied Scientist role will design and implement state-of-the-art AI optimization models that generate multi-billion dollar predictions of the highest level of visibility and importance for Amazon's Payments and Customer Experience. The scientist will work on implementing Agentic workflows within production systems that will tremendously improve developer productivity at scale. A successful candidate will be a problem solver who enjoys diving into data, is excited by difficult modeling challenges, and possesses strong communication skills to effectively interface between technical and business teams. You will contribute to the research community by working with other scientists across Amazon and our Payments Engineering as well as by collaborating with academic researchers and publishing papers. You will work closely with Software Development Engineers to invent and construct models on data at massive scale and it is likely that your work will end up in an Amazon product. Finally, you will also have exposure to senior leadership as we communicate results and provide scientific guidance to the business.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "Ia_f1QvLzJp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCwoh9x_zODB",
        "outputId": "ed607b8e-01fe-4176-ac99-d6153b2fc97d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-u11P1FzQ1V",
        "outputId": "b5a7681c-cc1b-4195-a5ee-eeb7dff7dad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpcRI9TkzTy_",
        "outputId": "475ad0d8-0cd0-4fe8-89ac-5beb10a81d22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text.lower())\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    clean_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
        "    return ' '.join(clean_tokens)"
      ],
      "metadata": {
        "id": "mTrhcTR7zVvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert dict values to list of strings and concatenate with the other list\n",
        "exp_texts = []\n",
        "if isinstance(json_job[\"experience\"], dict):\n",
        "    exp_texts = list(json_job[\"experience\"].values())\n",
        "elif isinstance(json_job[\"experience\"], list):\n",
        "    exp_texts = json_job[\"experience\"]\n",
        "\n",
        "skill_texts = []\n",
        "if isinstance(json_job[\"skills\"], dict):\n",
        "    skill_texts = list(json_job[\"skills\"].values())\n",
        "elif isinstance(json_job[\"skills\"], list):\n",
        "    skill_texts = json_job[\"skills\"]\n",
        "\n",
        "# Flatten nested lists if any\n",
        "flat_exp_texts = []\n",
        "for item in exp_texts:\n",
        "    if isinstance(item, list):\n",
        "        flat_exp_texts.extend(item)\n",
        "    else:\n",
        "        flat_exp_texts.append(item)\n",
        "\n",
        "flat_skill_texts = []\n",
        "for item in skill_texts:\n",
        "    if isinstance(item, list):\n",
        "        flat_skill_texts.extend(item)\n",
        "    else:\n",
        "        flat_skill_texts.append(item)\n",
        "\n",
        "# Ensure all elements are strings before joining\n",
        "string_exp_texts = [str(item) for item in flat_exp_texts]\n",
        "string_skill_texts = [str(item) for item in flat_skill_texts]\n",
        "\n",
        "\n",
        "jd_text = preprocess_text(' '.join(string_exp_texts + string_skill_texts))"
      ],
      "metadata": {
        "id": "fe4ybMM5zbDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDJg3kx6z2Ce",
        "outputId": "c2960af7-819d-4cc4-805b-dae1e9d61628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jd_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "nnvzpd_-z6wO",
        "outputId": "11934abd-8755-49d5-f466-4596eeeaabcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'master degree computer science mathematics statistic machine learning equivalent quantitative field experience programming java python related language experience sql rdbms oracle data warehouse experience building machine learning model developing algorithm business application experience implementing algorithm using toolkits code publication conference journal phd computer science mathematics statistic machine learning equivalent quantitative field experience solving business problem machine learning data mining statistical algorithm machine learning deep learning python java sql data warehouse rdbms genai agentic workflow'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_text = preprocess_text(\n",
        "    ' '.join([\n",
        "        ' '.join([exp[\"position\"] + \" \" + exp[\"company\"] for exp in json_resume[\"experience\"]]),\n",
        "        ' '.join(json_resume[\"technical_skills\"][\"programming_languages\"]),\n",
        "        ' '.join(json_resume[\"technical_skills\"][\"tools\"]),\n",
        "        ' '.join(json_resume[\"technical_skills\"][\"coursework\"]),\n",
        "        ' '.join([\n",
        "            project[\"title\"] + ' ' + ' '.join(project[\"tech_used\"])\n",
        "            for project in json_resume.get(\"projects\", [])\n",
        "        ])\n",
        "    ])\n",
        ")"
      ],
      "metadata": {
        "id": "N_P2rDDD0RGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "m7umKBiw1YJj",
        "outputId": "7d9c7fcf-c413-49cb-9dca-25dbb846d84a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data science intern imarticus learning python java v code intellij idea eclipse pycharm jupyter notebook tableau powerbi data science statistic machine learning algorithm deep learning nlp data analysis data mining o data visualization predictive analytics modeling mysql aws sagemaker ec2 synchronizing supply chain data intelligence powerbi mysql etl excel ml model text sql python mysql server streamlit api satellite image classification using ml algorithm python machine learning image processing'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform([jd_text, resume_text])\n",
        "similarity_score = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]"
      ],
      "metadata": {
        "id": "fFkVNSvv1bR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ats_score = round(similarity_score * 100, 2)\n",
        "print(f\"✅ ATS Resume Match Score: {ats_score}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82Wyzs3U1fW3",
        "outputId": "848f1f0c-3d71-4a26-f5a6-67c08bf230f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ ATS Resume Match Score: 36.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "email_prompt = PromptTemplate.from_template(\"\"\"\n",
        "You are a professional email writer.\n",
        "\n",
        "Based on the following resume details and job description, write a formal job application email to apply for the job. Keep it polite, confident, and concise.\n",
        "\n",
        "### RESUME DETAILS (JSON):\n",
        "{resume_json}\n",
        "\n",
        "### JOB DESCRIPTION:\n",
        "{job_text}\n",
        "\n",
        "Write a complete email with a subject line, greeting, body, and a closing with the candidate’s name.\n",
        "\"\"\")\n",
        "email_chain = email_prompt | llm\n",
        "email_response = email_chain.invoke({\n",
        "    \"resume_json\": json_resume,\n",
        "    \"job_text\": page_data\n",
        "})\n",
        "\n",
        "print(email_response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzTlolEN1hoo",
        "outputId": "c7cd579c-1881-49a2-d5b7-71f3a9815fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is a formal job application email:\n",
            "\n",
            "Subject: Application for Applied Scientist I, Amazon Payments Team (Job ID: 2979106)\n",
            "\n",
            "Dear Hiring Manager,\n",
            "\n",
            "I am writing to express my strong interest in the Applied Scientist I position in the Amazon Payments Team (Job ID: 2979106). As a highly motivated and experienced data science professional, I am confident that my skills and expertise make me an ideal candidate for this role.\n",
            "\n",
            "With a background in Computer Science and Engineering (Data Science) from Presidency University, Bangalore, and a strong foundation in programming languages such as Python, Java, and HTML/CSS, I am well-equipped to tackle complex scientific optimization challenges. My recent internship experience as a Data Science Intern at Imarticus Learning has provided me with hands-on experience in building machine learning models, data analysis, and data visualization using tools like PowerBI, MySQL, and Jupyter Notebook.\n",
            "\n",
            "I am particularly drawn to this role because of the opportunity to design and implement state-of-the-art AI optimization models that generate multi-billion dollar predictions. My experience in solving business problems through machine learning, data mining, and statistical algorithms, as well as my proficiency in SQL and RDBMS, aligns well with the job requirements. I am excited about the prospect of working on impactful generative AI initiatives and driving reductions in operations.\n",
            "\n",
            "In addition to my technical skills, I possess excellent interpersonal skills, including communication, active listening, teamwork, and leadership. I have a proven track record of working effectively in fast-paced environments and collaborating with multiple teams.\n",
            "\n",
            "I have attached my resume, which provides more details about my education, experience, projects, and certifications. I would be thrilled to discuss my application and how I can contribute to the Amazon Payments Team.\n",
            "\n",
            "Thank you for considering my application. I look forward to the opportunity to discuss this role further.\n",
            "\n",
            "Best regards,\n",
            "\n",
            "Sainath Reddy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WDDdM-UI2ZTx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}